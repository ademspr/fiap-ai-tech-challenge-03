{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQUG7iSFiqAK"
   },
   "source": [
    "# FIAP - Fase 3\n",
    "This notebook will guide us on finetuning the chosen foundation model with specialized medical data using Google's Colab with L4 GPUs.\n",
    "\n",
    "## Prerequisites - Hugging Face Setup\n",
    "\n",
    "Before running this notebook, you need to setup access to Llama 3.2:\n",
    "\n",
    "1. Create a Hugging Face account\n",
    "2. Accept [Llama 3.2 license terms](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)\n",
    "3. Create [an access token](https://huggingface.co/settings/tokens) allowing \"Read access to contents of all public gated repos you can access\"\n",
    "4. Add the token to Colab Secrets\n",
    "\n",
    "## Download and run the model\n",
    "\n",
    "After the notebook finish run, the model will be saved in GGUF format at `/MyDrive/medqa-model/medqa.gguf` on your Google Drive. Download the `medqa.gguf` file from Drive to `outputs` folder on your local computer and execute the commands below to create the model into `ollama`:\n",
    "\n",
    "```bash\n",
    "ollama create medqa -f Modelfile\n",
    "ollama run medqa\n",
    "```\n",
    "\n",
    "**note:** you need to allow the notebook to connect to Google Drive to save the output model there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9LT0rUFHHgX"
   },
   "outputs": [],
   "source": [
    "# mount google drive to store trained model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DN8W4T2viqAM"
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install -q \\\n",
    "    torch==2.5.1 \\\n",
    "    torchvision==0.20.1 \\\n",
    "    torchaudio==2.5.1 \\\n",
    "    datasets==3.2.0 \\\n",
    "    transformers==4.46.3 \\\n",
    "    peft==0.14.0 \\\n",
    "    trl==0.12.2 \\\n",
    "    accelerate==1.2.1 \\\n",
    "    huggingface_hub==0.34.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama.cpp to convert to gguf\n",
    "!git clone --depth 1 https://github.com/ggerganov/llama.cpp\n",
    "!pip install -q ./llama.cpp/gguf-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to Hugging Face to be allowed to use chosen model\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNFdRTJciqAN"
   },
   "outputs": [],
   "source": [
    "# download dataset\n",
    "!mkdir -p data/pubmedqa/data\n",
    "!gdown --fuzzy 'https://drive.google.com/file/d/15v1x6aQDlZymaHGP7cZJZZYFfeJt2NdS/view' -O data/pubmedqa/data/ori_pqaa.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8Q_QxGL_fjh"
   },
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "import json\n",
    "import re\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a concise medical assistant.\n",
    "\n",
    "CRITICAL RULES - FOLLOW EXACTLY:\n",
    "- Maximum 2-3 sentences. NO EXCEPTIONS.\n",
    "- NO academic language. NO \"studies show\", \"research indicates\".\n",
    "- NO lists. NO bullet points. NO multiple paragraphs.\n",
    "- If answer needs more than 3 sentences, give only the most important point.\n",
    "- NEVER start with \"I\" or introduce yourself.\n",
    "- STOP writing after 3 sentences.\n",
    "- NEVER diagnose or prescribe medications.\n",
    "\n",
    "If unsure: \"I don't have reliable information. Please consult a healthcare professional.\\\"\"\"\"\n",
    "\n",
    "# patterns to filter from dataset\n",
    "REMOVE_PATTERNS = [\n",
    "    r\"Medical Subject Headings \\(MeSH\\):.*$\",\n",
    "    r\"To the best of our knowledge,?\\s*\",\n",
    "    r\"Our (findings|results|study) (suggest|show|indicate|implicate)\\s*\",\n",
    "    r\"These (findings|results) (suggest|raise|indicate)\\s*\",\n",
    "    r\"In (this|our) study,?\\s*\",\n",
    "    r\"Results depicted\\s*\",\n",
    "    r\"Â©.*$\", # Copyright\n",
    "    r\"Sincerely,.*$\",\n",
    "    r\"Best regards,.*$\",\n",
    "]\n",
    "\n",
    "def clean_text(text):\n",
    "    # clean academic language while preserving medical content\n",
    "    for pattern in REMOVE_PATTERNS:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "    # truncate long responses to teach conciseness\n",
    "    if len(text) > 450:\n",
    "        truncated = text[:400]\n",
    "        last_period = truncated.rfind('.')\n",
    "        if last_period > 100:\n",
    "            text = truncated[:last_period + 1]\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# load PubMedQA labeled dataset\n",
    "with open(\"data/pubmedqa/data/ori_pqaa.json\", \"r\") as f:\n",
    "    pubmedqa_data = json.load(f)\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for _, item in pubmedqa_data.items():\n",
    "    question = item.get(\"QUESTION\", \"\")\n",
    "    long_answer = item.get(\"LONG_ANSWER\", \"\")\n",
    "\n",
    "    if not question or not long_answer:\n",
    "        continue\n",
    "\n",
    "    dataset.append({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question.strip()},\n",
    "            {\"role\": \"assistant\", \"content\": clean_text(long_answer)}\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh-8TcwQHQjZ"
   },
   "outputs": [],
   "source": [
    "# refusal examples to teach model safety behaviors\n",
    "REFUSAL_EXAMPLES = [\n",
    "    # Diagnosis refusals\n",
    "    {\"question\": \"I have a headache, fever and body aches. What disease do I have?\",\n",
    "     \"answer\": \"These symptoms can be associated with various conditions. Please consult a healthcare professional for proper evaluation and diagnosis.\"},\n",
    "    {\"question\": \"My child has a rash and fever. Is it measles?\",\n",
    "     \"answer\": \"I cannot diagnose conditions. A rash with fever requires evaluation by a healthcare professional who can examine your child directly.\"},\n",
    "    {\"question\": \"I've been feeling dizzy and nauseous. What's wrong with me?\",\n",
    "     \"answer\": \"Dizziness and nausea can have many causes. Please consult a healthcare professional for proper evaluation.\"},\n",
    "    {\"question\": \"Based on my symptoms, do I have COVID-19?\",\n",
    "     \"answer\": \"I cannot diagnose COVID-19 or any other condition. Please get tested and consult a healthcare professional.\"},\n",
    "    {\"question\": \"I think I have diabetes because I'm always thirsty. Am I right?\",\n",
    "     \"answer\": \"Increased thirst can have various causes. Only a healthcare professional can diagnose diabetes through proper testing.\"},\n",
    "\n",
    "    # Prescription refusals\n",
    "    {\"question\": \"What medication should I take for my back pain?\",\n",
    "     \"answer\": \"I cannot recommend specific medications. Please consult a healthcare professional who can evaluate your condition and prescribe appropriate treatment.\"},\n",
    "    {\"question\": \"Can you prescribe me antibiotics for my infection?\",\n",
    "     \"answer\": \"I cannot prescribe medications. Antibiotics require a prescription from a licensed healthcare provider after proper evaluation.\"},\n",
    "    {\"question\": \"What dosage of ibuprofen should I take?\",\n",
    "     \"answer\": \"Medication dosages should be determined by a healthcare professional or pharmacist based on your specific situation.\"},\n",
    "    {\"question\": \"Should I take aspirin or acetaminophen for my headache?\",\n",
    "     \"answer\": \"I cannot recommend specific medications. Please consult a pharmacist or healthcare professional for appropriate recommendations.\"},\n",
    "    {\"question\": \"What's the best antidepressant for anxiety?\",\n",
    "     \"answer\": \"Mental health medications require evaluation by a qualified healthcare professional. Please consult a psychiatrist or your doctor.\"},\n",
    "\n",
    "    # Fake disease refusals\n",
    "    {\"question\": \"What are the symptoms of Hyperfluxitis?\",\n",
    "     \"answer\": \"I don't have reliable information about this condition. Please consult a healthcare professional or verify the condition name.\"},\n",
    "    {\"question\": \"Tell me about Neuroplastoma syndrome\",\n",
    "     \"answer\": \"I cannot find reliable information about this specific condition. Please consult a healthcare professional for accurate information.\"},\n",
    "    {\"question\": \"How is Cardiofibrillosis treated?\",\n",
    "     \"answer\": \"I don't have information about this condition. Please verify the name with a healthcare professional.\"},\n",
    "\n",
    "    # Emergency examples\n",
    "    {\"question\": \"I'm having chest pain right now, what should I do?\",\n",
    "     \"answer\": \"Chest pain can be serious. If you're experiencing chest pain, please seek immediate medical attention or call emergency services.\"},\n",
    "    {\"question\": \"I accidentally took too many pills, help!\",\n",
    "     \"answer\": \"This is a medical emergency. Please call poison control or emergency services immediately. Do not wait.\"},\n",
    "\n",
    "    # More diagnosis/prescription refusals\n",
    "    {\"question\": \"I have a lump in my breast, is it cancer?\",\n",
    "     \"answer\": \"I cannot diagnose whether a lump is cancerous. Please see a healthcare professional immediately for proper examination and testing.\"},\n",
    "    {\"question\": \"My blood pressure is 150/95, should I be worried?\",\n",
    "     \"answer\": \"I cannot interpret your blood pressure readings or provide medical advice. Please consult a healthcare professional for evaluation.\"},\n",
    "    {\"question\": \"What antibiotic works best for UTI?\",\n",
    "     \"answer\": \"I cannot recommend specific antibiotics. UTI treatment requires proper diagnosis and prescription from a healthcare provider.\"},\n",
    "    {\"question\": \"Can I stop taking my blood pressure medication?\",\n",
    "     \"answer\": \"Never stop or change medications without consulting your prescribing healthcare professional. This could be dangerous.\"},\n",
    "    {\"question\": \"Is my mole melanoma?\",\n",
    "     \"answer\": \"I cannot diagnose skin conditions. Any concerning mole should be examined by a dermatologist or healthcare professional.\"},\n",
    "    {\"question\": \"How much insulin should I inject?\",\n",
    "     \"answer\": \"Insulin dosing must be determined by your healthcare provider based on your specific condition. Never adjust doses without medical guidance.\"},\n",
    "]\n",
    "\n",
    "# add refusal examples to dataset (4 copies each to increase weight)\n",
    "for example in REFUSAL_EXAMPLES:\n",
    "    for _ in range(4):\n",
    "        dataset.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "                {\"role\": \"assistant\", \"content\": example[\"answer\"]}\n",
    "            ]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoM-fyJ2_hjC"
   },
   "outputs": [],
   "source": [
    "# save prepared dataset\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def save_jsonl(data, path):\n",
    "    Path(path).parent.mkdir(exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        f.writelines(json.dumps(item, ensure_ascii=False) + '\\n' for item in data)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "split_index = int(len(dataset) * 0.9)\n",
    "save_jsonl(dataset[:split_index], \"datasets/train.jsonl\")\n",
    "save_jsonl(dataset[split_index:], \"datasets/validation.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nsB7RFQA3r1"
   },
   "outputs": [],
   "source": [
    "# fine tuning settings\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/fiap-3-model\"\n",
    "\n",
    "# LoRA\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# training\n",
    "MAX_SEQ_LENGTH = 400\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 36\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuceWvzTiqAP"
   },
   "outputs": [],
   "source": [
    "# setup memory management\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cG08kjHlBClH"
   },
   "outputs": [],
   "source": [
    "# load model in fp16 (no quantization to support direct GGUF conversion)\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avChK7AKiqAP"
   },
   "outputs": [],
   "source": [
    "# setup Lora\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aODq6DtdiqAQ"
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "from datasets import load_dataset\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=\"datasets/train.jsonl\", split=\"train\")\n",
    "validation_dataset = load_dataset(\"json\", data_files=\"datasets/validation.jsonl\", split=\"train\")\n",
    "\n",
    "def format_messages(examples):\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "train_dataset = train_dataset.map(format_messages, batched=True, remove_columns=train_dataset.column_names)\n",
    "validation_dataset = validation_dataset.map(format_messages, batched=True, remove_columns=validation_dataset.column_names)\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h50guwwaLCi0"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=SFTConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_ratio=0.05,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=100,\n",
    "        save_steps=100,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        gradient_checkpointing=False,\n",
    "        max_grad_norm=1.0,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG0K9mMxqrqB"
   },
   "outputs": [],
   "source": [
    "# save trained model\n",
    "trained_model = trainer.model\n",
    "merged_model = trained_model.merge_and_unload()\n",
    "merged_model.save_pretrained(f\"{OUTPUT_DIR}/merged_model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aX0bSLN0iqAQ"
   },
   "outputs": [],
   "source": [
    "# convert model to GGUF format for Ollama compatibility\n",
    "!python llama.cpp/convert_hf_to_gguf.py {OUTPUT_DIR}/merged_model --outfile {OUTPUT_DIR}/model.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LD2D96r5r3y5"
   },
   "outputs": [],
   "source": [
    "# end runtime to prevent wasting credits\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
